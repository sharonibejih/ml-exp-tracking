{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning and Experiment Tracking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> __Setup__:\n",
    "> For Windows users, install git bash if you do not have that already.<br><br>\n",
    "> Open Git Bash/Mac Terminal and do the following:\n",
    "> - Create a Python environment: `conda create -n env-name` or `python -m venv env-name`\n",
    "> - Activate the environment: `conda activate env-name` or `source env-name/Scripts/activate`.\n",
    "> - Clone the lab's repository: `git clone https://github.com/sharonibejih/ml-exp-tracking.git`\n",
    "> - Change current working directory: `cd ml-exp-tracking`<br><br>\n",
    "> Open your Jupyter Notebook locally (i.e from Anaconda, VSCode or any other IDE that you can access locally, which you are comfortable with).<br><br>\n",
    "> Ensure that the location of the notebook in inside the \"ml-exp-tracking\" folder.<br><br>\n",
    "> _P.S: It's okay to choose to work with the notebook that already exist in the cloned repo._\n",
    "\n",
    "\n",
    "__Machine Learning:__\n",
    "\n",
    "In the simplest term, machine learning is making a computer learn to do what a human can do. <br><br>\n",
    "\n",
    ">According to Chip Huyen, Machine learning is an approach to learn complex patterns from existing data and use these patterns to make predictions on unseen data.<br><br>\n",
    "\n",
    "Since learning is a process for us as humans, it is also a process for machines. To determin whether a model has learnt well enough, we evaluate them either with qualitative measures, quanitative or both. \n",
    "\n",
    "<img src=\"ml_vs_traditional_paradigm.png\">\n",
    "\n",
    "For each time we train a model and evaluate, we carry out a machine learning experiment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experiment Tracking\n",
    "\n",
    "Just like in our science labs for example, we put down observations from any experiments carried out. The same practice is extremely required in machine learning too. \n",
    "\n",
    "Reviewing past experiments, gives insights on what performed best and possible reasons why some experiments failed. It also gives the ability to reproduce past experiment or share findings with the team.\n",
    "\n",
    "\n",
    "When building machine learning models, we try a lot of things to be able to arrive at an acceptable model. Some of the things we try could be the data preprocessing technique, the data split size, the hyperparameters and the machine learning algorithm used. \n",
    "\n",
    "\n",
    "> Machine learning Experiment tracking is the process of recording all these [important] details (called metadata) that associates with any trained model, including their performance scores.\n",
    "\n",
    "\n",
    "#### Why is ML Experiment Tracking Important?\n",
    "\n",
    "What happens if your team lead asks you to use a model you trained a month ago? Can you reproduce that model if it isn't saved to a path? And even if it was saved can you give details on the model if you never recorded them somewhere else other than your notebook, which may no longer be available?\n",
    "\n",
    "Perhaps all of these could give you insights on why documentation is important.\n",
    "\n",
    "\n",
    "__Tracking ML Experiments__\n",
    "<br><br>\n",
    "<img src=\"tracking_with_spreadsheet.png\">\n",
    "\n",
    "> <br>\n",
    "> Class Question: <br>\n",
    "> Do you see any problems with this method of tracking results?\n",
    "> <br><br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MLflow\n",
    "\n",
    "Think of MLflow has a platform that helps you manage your machine learning cycle - experimentation, deployment, and model storage. The goal is to aid reproducibility, a key ingredient in research and engineering.\n",
    "\n",
    "Today, we will focus on the experiment tracking feature of MLflow and see how it can be used.\n",
    "\n",
    "<img src=\"gui_result.png\">\n",
    "\n",
    "REF: https://mlflow.org/docs/latest/tracking.html \n",
    "\n",
    "\n",
    "#### Track your ML Experiments in 3 Basic Steps:\n",
    "1. Create the experiment by giving it a name: `mlflow.set_experiment(\"experiment-name\")`<br><br>\n",
    "2. Set the Tracking URI: `mlflow.set_tracking_uri()`<br><br>\n",
    "3. Log the metadata and artifacts for each run: `mlflow.start_run`<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__DEFINITION OF TERMS__\n",
    "\n",
    "__Experiment__ in MLflow is simply what we use to represent the name of the project, for example: \"fraud-detection\" or \"term-deposit\".\n",
    "\n",
    "__Run__ represents each modeling carried out under this project (or experiment). Just like trying out three or four fraud detection models before concluding on the best. Each try is called a run. After this is executed, it stores the metadata and artifacts of the model.\n",
    "\n",
    "__Metadata__ is the light details of the model such as metrics, tags, hyperparemeter values, etc. What metadata is store is your choice.\n",
    "\n",
    "__Artifacts__ are the heavier results of the model such as the preprocessor, the exported model, images, data etc. What artifacts to store is also based on choice.\n",
    "\n",
    "__Tracking URI__ is an optional choice. Tracking URI is used if :\n",
    "- you want to store your files (metadata and artifacts) to a specific local directory.\n",
    "- you want to log your results to a specific resource, say to a specific host. <br><br>If tracking uri is specified without stating the file path or HTTP URI, then, it will use the default: __Files will be stored in your working directory by creating a folder called `mlruns` and the UI will be accessible via `http://127.0.0.1:5000`.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, let's code!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## pip install mlflow\n",
    "## pip install xgboost\n",
    "\n",
    "### Ctrl + Shift + P to add env to Jupyter Notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction import DictVectorizer\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "import pickle # to export and load the model\n",
    "\n",
    "import mlflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ML model has been created before now, using the following simple codes. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_path:str, test_path:str):\n",
    "    train = pd.read_csv(train_path, sep=\";\")\n",
    "    test = pd.read_csv(test_path, sep=\";\")\n",
    "    \n",
    "    return train, test\n",
    "\n",
    "    \n",
    "def preprocess_data(train_data, test_data):\n",
    "\n",
    "    dv = DictVectorizer()\n",
    "\n",
    "    trainX = train_data.drop(columns=[\"y\"])\n",
    "    trainy = train_data[\"y\"]\n",
    "\n",
    "    testX = test_data.drop(columns=[\"y\"])\n",
    "    testy = test_data[\"y\"]\n",
    "\n",
    "    trainy = trainy.replace({\"no\":0, \"yes\": 1})\n",
    "    testy = testy.replace({\"no\":0, \"yes\": 1})\n",
    "\n",
    "    trainX = dv.fit_transform(trainX.to_dict(orient=\"records\")).toarray()\n",
    "\n",
    "    testX = dv.fit_transform(testX.to_dict(orient=\"records\")).toarray()\n",
    "    \n",
    "    return dv, trainX, trainy, testX, testy\n",
    "\n",
    "\n",
    "def train_model(trainX, trainy, testX):\n",
    "\n",
    "    xgb = XGBClassifier()\n",
    "    \n",
    "    xgb.fit(trainX, trainy)\n",
    "    \n",
    "    pred = xgb.predict(testX)\n",
    "\n",
    "    return xgb, pred\n",
    "    \n",
    "\n",
    "def evaluate(testy, pred):\n",
    "    \n",
    "    return (f1_score(testy, pred))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "3f1d252ef46601678906dbdce4acb4f440340dbfaa288531fe6c2b835d5664f6"
  },
  "kernelspec": {
   "display_name": "Python 3.10.9 ('ml-exp-env')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
